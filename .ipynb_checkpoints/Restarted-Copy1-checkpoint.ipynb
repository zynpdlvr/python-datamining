{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7db9985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2675bd3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_excel('Course Project - Data for Classification - Electric Vehicles.xls')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7593525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23070e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df['Q16'].unique()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f0af2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Q16'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5a39c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd25157f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q11_1</th>\n",
       "      <th>Q11_2</th>\n",
       "      <th>Q11_3</th>\n",
       "      <th>Q11_4</th>\n",
       "      <th>Q11_5</th>\n",
       "      <th>Q11_6</th>\n",
       "      <th>Q11_7</th>\n",
       "      <th>Q11_8</th>\n",
       "      <th>Q11_9</th>\n",
       "      <th>Q11_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Q18_17</th>\n",
       "      <th>Q18_18</th>\n",
       "      <th>Q18_19</th>\n",
       "      <th>Q18_20</th>\n",
       "      <th>Q18_21</th>\n",
       "      <th>Q18_22</th>\n",
       "      <th>Q18_23</th>\n",
       "      <th>Q20</th>\n",
       "      <th>Q21</th>\n",
       "      <th>Q16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Q11_1  Q11_2  Q11_3  Q11_4  Q11_5  Q11_6  Q11_7  Q11_8  Q11_9  Q11_10  ...  \\\n",
       "0      0      0      0      1      1      0      0      1      0       0  ...   \n",
       "1      1      0      0      1      0      0      1      0      0       1  ...   \n",
       "2      1      1      0      1      0      0      1      1      0       1  ...   \n",
       "3      0      0      0      0      0      0      0      0      0       0  ...   \n",
       "4      1      1      0      1      0      0      1      1      0       1  ...   \n",
       "\n",
       "   Q18_17  Q18_18  Q18_19  Q18_20  Q18_21  Q18_22  Q18_23  Q20  Q21  Q16  \n",
       "0     5.0     5.0     5.0     5.0     5.0     5.0     5.0    3    3    A  \n",
       "1     4.0     2.0     4.0     2.0     2.0     2.0     2.0    3    3    D  \n",
       "2     4.0     4.0     NaN     NaN     4.0     2.0     4.0    4    1    A  \n",
       "3     NaN     NaN     NaN     NaN     NaN     NaN     NaN    3    3    C  \n",
       "4     NaN     NaN     1.0     1.0     NaN     NaN     NaN    3    3    D  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace non-numeric values with NaN\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36ed904e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.impute import KNNImputer\\n\\nexclude_column_name = 'Q16'\\ncolumn_to_exclude = df[exclude_column_name]\\n\\n# Create a list of k values\\nk_values = [1, 3, 5, 7, 9,11]\\n\\n# Loop through each k value\\nfor k in k_values:\\n    # Drop the column you want to exclude\\n    columns_to_impute = df.drop(columns=[exclude_column_name])\\n    \\n    # Apply KNN imputation for the current k value\\n    imputer = KNNImputer(n_neighbors=k)\\n    imputed_data = imputer.fit_transform(columns_to_impute)\\n    \\n    \\n    imputed_df = pd.DataFrame(imputed_data, columns=columns_to_impute.columns)\\n    imputed_df[exclude_column_name] = column_to_exclude\\n\\nimputed_df.head()\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.impute import KNNImputer\n",
    "\n",
    "exclude_column_name = 'Q16'\n",
    "column_to_exclude = df[exclude_column_name]\n",
    "\n",
    "# Create a list of k values\n",
    "k_values = [1, 3, 5, 7, 9,11]\n",
    "\n",
    "# Loop through each k value\n",
    "for k in k_values:\n",
    "    # Drop the column you want to exclude\n",
    "    columns_to_impute = df.drop(columns=[exclude_column_name])\n",
    "    \n",
    "    # Apply KNN imputation for the current k value\n",
    "    imputer = KNNImputer(n_neighbors=k)\n",
    "    imputed_data = imputer.fit_transform(columns_to_impute)\n",
    "    \n",
    "    \n",
    "    imputed_df = pd.DataFrame(imputed_data, columns=columns_to_impute.columns)\n",
    "    imputed_df[exclude_column_name] = column_to_exclude\n",
    "\n",
    "imputed_df.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "273caba3-4506-4a95-9c97-086c89b07519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Replace '?' with NaN for proper handling\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "original_df = df.copy()\n",
    "# Specify the imputation strategy (mean or median)\n",
    "strategy = 'mean'  # or 'median'\n",
    "df = df.drop(columns=['Q16'])\n",
    "# Create the imputer\n",
    "imputer = SimpleImputer(strategy=strategy)\n",
    "\n",
    "# Apply imputation\n",
    "imputed_data = imputer.fit_transform(df)\n",
    "\n",
    "# Convert the result back to a DataFrame\n",
    "imputed_df = pd.DataFrame(imputed_data, columns=df.columns)\n",
    "imputed_df['Q16'] = original_df['Q16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe1c6427-068b-4fa1-96bb-bad6d7fcb6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Features:\n",
      "Q17       0.208779\n",
      "Q14       0.198048\n",
      "Q18_14    0.149196\n",
      "Q18_1     0.146733\n",
      "Q21       0.120714\n",
      "            ...   \n",
      "Q13_4     0.000000\n",
      "Q12_13    0.000000\n",
      "Q12_8     0.000000\n",
      "Q11_8     0.000000\n",
      "Q11_6     0.000000\n",
      "Length: 74, dtype: float64\n",
      "Ranked Features:\n",
      "Q17              0.208779\n",
      "Q14              0.198048\n",
      "Q18_14           0.149196\n",
      "Q18_1            0.146733\n",
      "Q21              0.120714\n",
      "Q12_14           0.119682\n",
      "Q13_16           0.111039\n",
      "Q18_8            0.099074\n",
      "Q18_13           0.092218\n",
      "Q18_9            0.090830\n",
      "Q18_2            0.080725\n",
      "Q18_10           0.074609\n",
      "Q18_15           0.072366\n",
      "Q18_16           0.070297\n",
      "Q18_6            0.067568\n",
      "Q18_3            0.067343\n",
      "Q18_4            0.064563\n",
      "Q18_17           0.063939\n",
      "Q18_19           0.063002\n",
      "Q18_23           0.062229\n",
      "Q18_20           0.061165\n",
      "Q18_7            0.060824\n",
      "Q18_18           0.059437\n",
      "Q18_21           0.053193\n",
      "Q12_4            0.052705\n",
      "Q18_11           0.051565\n",
      "Q13_Dont_Know    0.049260\n",
      "Q18_22           0.048928\n",
      "Q18_5            0.048681\n",
      "Q13_2            0.047018\n",
      "Q18_12           0.041222\n",
      "Q15              0.033848\n",
      "Q13_11           0.030045\n",
      "Q12_10           0.028639\n",
      "Q11_5            0.027568\n",
      "Q12_2            0.027364\n",
      "Q13_3            0.026563\n",
      "Q13_6            0.023250\n",
      "Q11_Dont_Know    0.023166\n",
      "Q12_3            0.023013\n",
      "Q12_5            0.022563\n",
      "Q20              0.021525\n",
      "Q12_Dont_Know    0.020975\n",
      "Q12_1            0.019900\n",
      "Q13_5            0.019735\n",
      "Q12_7            0.019563\n",
      "Q13_7            0.017374\n",
      "Q13_14           0.016411\n",
      "Q13_1            0.014670\n",
      "Q11_10           0.013016\n",
      "Q11_3            0.012239\n",
      "Q11_7            0.010904\n",
      "Q12_11           0.009023\n",
      "Q11_11           0.008359\n",
      "Q12_6            0.007456\n",
      "Q11_13           0.007151\n",
      "Q11_4            0.006708\n",
      "Q13_12           0.006362\n",
      "Q13_9            0.006261\n",
      "Q12_12           0.005913\n",
      "Q13_13           0.004513\n",
      "Q11_1            0.002140\n",
      "Q12_9            0.001900\n",
      "Q11_9            0.001246\n",
      "Q11_12           0.001014\n",
      "Q13_15           0.000000\n",
      "Q13_10           0.000000\n",
      "Q11_2            0.000000\n",
      "Q13_8            0.000000\n",
      "Q13_4            0.000000\n",
      "Q12_13           0.000000\n",
      "Q12_8            0.000000\n",
      "Q11_8            0.000000\n",
      "Q11_6            0.000000\n",
      "dtype: float64\n",
      "Non-zero Mutual Information Scores for Features:\n",
      "Q17              0.208779\n",
      "Q14              0.198048\n",
      "Q18_14           0.149196\n",
      "Q18_1            0.146733\n",
      "Q21              0.120714\n",
      "Q12_14           0.119682\n",
      "Q13_16           0.111039\n",
      "Q18_8            0.099074\n",
      "Q18_13           0.092218\n",
      "Q18_9            0.090830\n",
      "Q18_2            0.080725\n",
      "Q18_10           0.074609\n",
      "Q18_15           0.072366\n",
      "Q18_16           0.070297\n",
      "Q18_6            0.067568\n",
      "Q18_3            0.067343\n",
      "Q18_4            0.064563\n",
      "Q18_17           0.063939\n",
      "Q18_19           0.063002\n",
      "Q18_23           0.062229\n",
      "Q18_20           0.061165\n",
      "Q18_7            0.060824\n",
      "Q18_18           0.059437\n",
      "Q18_21           0.053193\n",
      "Q12_4            0.052705\n",
      "Q18_11           0.051565\n",
      "Q13_Dont_Know    0.049260\n",
      "Q18_22           0.048928\n",
      "Q18_5            0.048681\n",
      "Q13_2            0.047018\n",
      "Q18_12           0.041222\n",
      "Q15              0.033848\n",
      "Q13_11           0.030045\n",
      "Q12_10           0.028639\n",
      "Q11_5            0.027568\n",
      "Q12_2            0.027364\n",
      "Q13_3            0.026563\n",
      "Q13_6            0.023250\n",
      "Q11_Dont_Know    0.023166\n",
      "Q12_3            0.023013\n",
      "Q12_5            0.022563\n",
      "Q20              0.021525\n",
      "Q12_Dont_Know    0.020975\n",
      "Q12_1            0.019900\n",
      "Q13_5            0.019735\n",
      "Q12_7            0.019563\n",
      "Q13_7            0.017374\n",
      "Q13_14           0.016411\n",
      "Q13_1            0.014670\n",
      "Q11_10           0.013016\n",
      "Q11_3            0.012239\n",
      "Q11_7            0.010904\n",
      "Q12_11           0.009023\n",
      "Q11_11           0.008359\n",
      "Q12_6            0.007456\n",
      "Q11_13           0.007151\n",
      "Q11_4            0.006708\n",
      "Q13_12           0.006362\n",
      "Q13_9            0.006261\n",
      "Q12_12           0.005913\n",
      "Q13_13           0.004513\n",
      "Q11_1            0.002140\n",
      "Q12_9            0.001900\n",
      "Q11_9            0.001246\n",
      "Q11_12           0.001014\n",
      "dtype: float64\n",
      "        Q17       Q14    Q18_14     Q18_1  Q21  Q12_14  Q13_16     Q18_8  \\\n",
      "0  3.000000  1.000000  5.000000  5.000000  3.0     0.0     0.0  5.000000   \n",
      "1  3.000000  3.000000  4.000000  4.000000  3.0     0.0     0.0  3.000000   \n",
      "2  1.000000  1.000000  4.000000  5.000000  1.0     0.0     0.0  4.000000   \n",
      "3  2.182072  2.383222  3.280962  3.479726  3.0     1.0     1.0  3.185137   \n",
      "4  2.182072  2.383222  3.280962  3.479726  3.0     0.0     0.0  3.185137   \n",
      "\n",
      "     Q18_13     Q18_9  ...  Q11_4  Q13_12  Q13_9  Q12_12  Q13_13  Q11_1  \\\n",
      "0  5.000000  5.000000  ...    1.0     0.0    1.0     0.0     1.0    0.0   \n",
      "1  2.000000  3.000000  ...    1.0     0.0    1.0     0.0     0.0    1.0   \n",
      "2  2.507821  4.000000  ...    1.0     0.0    1.0     0.0     0.0    1.0   \n",
      "3  2.507821  3.467643  ...    0.0     0.0    0.0     0.0     0.0    0.0   \n",
      "4  2.507821  3.467643  ...    1.0     0.0    0.0     0.0     0.0    1.0   \n",
      "\n",
      "   Q12_9  Q11_9  Q11_12  Q16  \n",
      "0    0.0    0.0     0.0    A  \n",
      "1    0.0    0.0     0.0    D  \n",
      "2    0.0    0.0     0.0    A  \n",
      "3    0.0    0.0     0.0    C  \n",
      "4    0.0    0.0     0.0    D  \n",
      "\n",
      "[5 rows x 66 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "###MUTUAL INFO\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "target_column = 'Q16'\n",
    "threshold = 0.01  # Set your desired correlation threshold\n",
    "\n",
    "# Copy the DataFrame to avoid modifying the original\n",
    "imputed_df_copy = imputed_df.copy()\n",
    "\n",
    "# Use mutual_info_classif for feature selection with categorical target\n",
    "X = imputed_df_copy.drop(columns=[target_column])\n",
    "y = imputed_df_copy[target_column]\n",
    "\n",
    "# Calculate mutual information between features and target\n",
    "mutual_info_values = mutual_info_classif(X, y)\n",
    "\n",
    "# Create a Series with feature names and their mutual information scores\n",
    "feature_mutual_info = pd.Series(mutual_info_values, index=X.columns)\n",
    "\n",
    "# Select features based on the mutual information threshold\n",
    "selected_features = feature_mutual_info[feature_mutual_info > threshold].index.tolist()\n",
    "\n",
    "# Rank features by mutual information scores\n",
    "ranked_features = feature_mutual_info.sort_values(ascending=False)\n",
    "\n",
    "# Display the ranked features\n",
    "print(\"Ranked Features:\")\n",
    "print(ranked_features)\n",
    "\n",
    "# Set pandas display options to show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Print the ranked features\n",
    "print(\"Ranked Features:\")\n",
    "print(ranked_features)\n",
    "\n",
    "\n",
    "non_zero_features = ranked_features[ranked_features >= 0.001]\n",
    "\n",
    "# Display non-zero features and their mutual information scores\n",
    "print(\"Non-zero Mutual Information Scores for Features:\")\n",
    "print(non_zero_features)\n",
    "pd.reset_option('display.max_rows')\n",
    "\n",
    "non_zero_columns = non_zero_features.index.tolist()\n",
    "\n",
    "# Create a new DataFrame with only the non-zero columns\n",
    "new_df = imputed_df[non_zero_columns].copy()\n",
    "new_df['Q16'] = imputed_df['Q16']\n",
    "print(new_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0883737f-abc1-493c-9374-b70638af2cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q17</th>\n",
       "      <th>Q14</th>\n",
       "      <th>Q18_14</th>\n",
       "      <th>Q18_1</th>\n",
       "      <th>Q21</th>\n",
       "      <th>Q12_14</th>\n",
       "      <th>Q13_16</th>\n",
       "      <th>Q18_8</th>\n",
       "      <th>Q18_13</th>\n",
       "      <th>Q18_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Q11_4</th>\n",
       "      <th>Q13_12</th>\n",
       "      <th>Q13_9</th>\n",
       "      <th>Q12_12</th>\n",
       "      <th>Q13_13</th>\n",
       "      <th>Q11_1</th>\n",
       "      <th>Q12_9</th>\n",
       "      <th>Q11_9</th>\n",
       "      <th>Q11_12</th>\n",
       "      <th>Q16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Q17  Q14  Q18_14  Q18_1  Q21  Q12_14  Q13_16  Q18_8  Q18_13  Q18_9  ...  \\\n",
       "0  0.0  0.0     1.0    1.0  0.0     0.0     0.0    1.0     1.0    1.0  ...   \n",
       "1  0.0  0.0     0.0    0.0  0.0     0.0     0.0    0.0     0.0    0.0  ...   \n",
       "2  0.0  0.0     0.0    1.0  0.0     0.0     0.0    0.0     0.0    0.0  ...   \n",
       "3  0.0  0.0     0.0    0.0  0.0     1.0     1.0    0.0     0.0    0.0  ...   \n",
       "4  0.0  0.0     0.0    0.0  0.0     0.0     0.0    0.0     0.0    0.0  ...   \n",
       "\n",
       "   Q11_4  Q13_12  Q13_9  Q12_12  Q13_13  Q11_1  Q12_9  Q11_9  Q11_12  Q16  \n",
       "0    1.0     0.0    1.0     0.0     1.0    0.0    0.0    0.0     0.0    A  \n",
       "1    1.0     0.0    1.0     0.0     0.0    1.0    0.0    0.0     0.0    D  \n",
       "2    1.0     0.0    1.0     0.0     0.0    1.0    0.0    0.0     0.0    A  \n",
       "3    0.0     0.0    0.0     0.0     0.0    0.0    0.0    0.0     0.0    C  \n",
       "4    1.0     0.0    0.0     0.0     0.0    1.0    0.0    0.0     0.0    D  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()\n",
    "scaled_columns = new_df.iloc[:, :-1].apply(lambda x: (x - x.min()) / (x.max() - x.min()) if pd.api.types.is_numeric_dtype(x) else x, axis=0)\n",
    "\n",
    "# Concatenate the original DataFrame with the scaled columns\n",
    "new_df = pd.concat([scaled_columns, new_df['Q16']], axis=1)\n",
    "\n",
    "# Convert values to binary (0 or 1) based on the condition\n",
    "new_df.iloc[:, :-1] = new_df.iloc[:, :-1].apply(lambda x: x.map(lambda val: 1 if pd.notna(val) and float(val) > 0.80 else 0))\n",
    "\n",
    "# Convert to integer to make them binary\n",
    "new_df.iloc[:, :-1] = new_df.iloc[:, :-1].astype(int)\n",
    "\n",
    "\n",
    "new_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24f6f6ad-7ec5-488b-a152-3c9c0319ecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7202\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, RobustScaler\n",
    "from scipy import stats\n",
    "\n",
    "# Ã–zellik ve hedef deÄŸiÅŸkeni seÃ§\n",
    "X = imputed_df.drop('Q16', axis=1)\n",
    "y = imputed_df['Q16']\n",
    "\n",
    "# AykÄ±rÄ± deÄŸerleri ele //\n",
    "z_scores = stats.zscore(X)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "X = X[filtered_entries]\n",
    "y = y[filtered_entries]\n",
    "\n",
    "# GeniÅŸletilmiÅŸ Ã¶zellik setini oluÅŸtur\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Ã–zellikleri RobustScaler ile Ã¶lÃ§eklendirme\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X_poly)\n",
    "\n",
    "# Veriyi train ve test setlerine ayÄ±rma\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Create RandomForestClassifier with the best parameters\n",
    "rf_classifier = RandomForestClassifier(max_depth= 10, min_samples_leaf= 2, min_samples_split= 5, n_estimators= 200)\n",
    "\n",
    "# Modeli eÄŸitin\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Test seti Ã¼zerinde tahminler yapÄ±n\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# DoÄŸruluk skorunu deÄŸerlendirin\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5e88f40-05bc-4e4c-a72f-2e0214a31c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.preprocessing import PolynomialFeatures\\nfrom sklearn.feature_selection import RFECV\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split, cross_val_score\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Ã–zellik ve hedef deÄŸiÅŸkeni seÃ§\\nX = imputed_df.drop(\\'Q16\\', axis=1)\\ny = imputed_df[\\'Q16\\']\\n\\n# AykÄ±rÄ± deÄŸerleri ele\\nz_scores = stats.zscore(X)\\nabs_z_scores = np.abs(z_scores)\\nfiltered_entries = (abs_z_scores < 3).all(axis=1)\\nX = X[filtered_entries]\\ny = y[filtered_entries]\\n\\n# GeniÅŸletilmiÅŸ Ã¶zellik setini oluÅŸtur\\npoly = PolynomialFeatures(degree=2)\\nX_poly = poly.fit_transform(X)\\n\\n# Ã–zellikleri RobustScaler ile Ã¶lÃ§eklendirme\\nscaler = StandardScaler()\\nX_scaled = scaler.fit_transform(X_poly)\\n\\n# Veriyi train ve test setlerine ayÄ±rma\\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\\n\\n# Logistic Regression modeli\\nlogreg = LogisticRegression(max_iter=1000)\\n\\n# RFECV\\'yi kullanarak Ã¶zellik seÃ§imi yapÄ±n\\nrfecv = RFECV(estimator=logreg, step=1, cv=5, scoring=\\'accuracy\\')  # cv=5, 5-fold cross-validation kullanÄ±yoruz\\nX_train_rfecv = rfecv.fit_transform(X_train, y_train)\\n\\n# Modeli eÄŸitin\\nlogreg.fit(X_train_rfecv, y_train)\\n\\n# Test seti Ã¼zerinde tahminler yapÄ±n\\nX_test_rfecv = rfecv.transform(X_test)\\ny_pred = logreg.predict(X_test_rfecv)\\n\\n# DoÄŸruluk skorunu deÄŸerlendirin\\naccuracy = accuracy_score(y_test, y_pred)\\nprint(f\"Logistic Regression Model Accuracy with RFECV: {accuracy:.4f}\")\\n\\n# Modelin performansÄ±nÄ± 5 katlÄ± Ã§apraz doÄŸrulama ile deÄŸerlendirin\\ncv_scores = cross_val_score(logreg, X_train_rfecv, y_train, cv=5, scoring=\\'accuracy\\')\\n\\n# Her bir Ã§apraz doÄŸrulama seti iÃ§in doÄŸruluk skorlarÄ±nÄ± yazdÄ±r\\nfor i, score in enumerate(cv_scores, start=1):\\n    print(f\"Ã‡apraz DoÄŸrulama Seti {i}: DoÄŸruluk = {score}\")\\n\\n# Ortalama doÄŸruluk skorunu hesapla\\nmean_accuracy = cv_scores.mean()\\n\\nprint(\"\\n5 KatlÄ± Ã‡apraz DoÄŸrulama Ä°le Ortalama DoÄŸruluk:\", mean_accuracy)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ã–zellik ve hedef deÄŸiÅŸkeni seÃ§\n",
    "X = imputed_df.drop('Q16', axis=1)\n",
    "y = imputed_df['Q16']\n",
    "\n",
    "# AykÄ±rÄ± deÄŸerleri ele\n",
    "z_scores = stats.zscore(X)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "X = X[filtered_entries]\n",
    "y = y[filtered_entries]\n",
    "\n",
    "# GeniÅŸletilmiÅŸ Ã¶zellik setini oluÅŸtur\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Ã–zellikleri RobustScaler ile Ã¶lÃ§eklendirme\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_poly)\n",
    "\n",
    "# Veriyi train ve test setlerine ayÄ±rma\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression modeli\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# RFECV'yi kullanarak Ã¶zellik seÃ§imi yapÄ±n\n",
    "rfecv = RFECV(estimator=logreg, step=1, cv=5, scoring='accuracy')  # cv=5, 5-fold cross-validation kullanÄ±yoruz\n",
    "X_train_rfecv = rfecv.fit_transform(X_train, y_train)\n",
    "\n",
    "# Modeli eÄŸitin\n",
    "logreg.fit(X_train_rfecv, y_train)\n",
    "\n",
    "# Test seti Ã¼zerinde tahminler yapÄ±n\n",
    "X_test_rfecv = rfecv.transform(X_test)\n",
    "y_pred = logreg.predict(X_test_rfecv)\n",
    "\n",
    "# DoÄŸruluk skorunu deÄŸerlendirin\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Model Accuracy with RFECV: {accuracy:.4f}\")\n",
    "\n",
    "# Modelin performansÄ±nÄ± 5 katlÄ± Ã§apraz doÄŸrulama ile deÄŸerlendirin\n",
    "cv_scores = cross_val_score(logreg, X_train_rfecv, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Her bir Ã§apraz doÄŸrulama seti iÃ§in doÄŸruluk skorlarÄ±nÄ± yazdÄ±r\n",
    "for i, score in enumerate(cv_scores, start=1):\n",
    "    print(f\"Ã‡apraz DoÄŸrulama Seti {i}: DoÄŸruluk = {score}\")\n",
    "\n",
    "# Ortalama doÄŸruluk skorunu hesapla\n",
    "mean_accuracy = cv_scores.mean()\n",
    "\n",
    "print(\"\\n5 KatlÄ± Ã‡apraz DoÄŸrulama Ä°le Ortalama DoÄŸruluk:\", mean_accuracy)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d767b487-8642-4e64-bd93-881c1e521545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Ã–nceden belirlenmiÅŸ X ve y'yi kullanalÄ±m\n",
    "# X: Ã–zellik matrisi, y: Hedef deÄŸiÅŸken\n",
    "# Ã–rneÄŸin, new_X ve new_y'yi kullanalÄ±m\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression modeli\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# RFECV'yi kullanarak Ã¶zellik seÃ§imi yapÄ±n\n",
    "rfecv = RFECV(estimator=logreg, step=1, cv=5, scoring='accuracy')  # cv=5, 5-fold cross-validation kullanÄ±yoruz\n",
    "X_train_rfecv = rfecv.fit_transform(X_train, y_train)\n",
    "\n",
    "# Modeli eÄŸitin\n",
    "logreg.fit(X_train_rfecv, y_train)\n",
    "\n",
    "# Test seti Ã¼zerinde tahminler yapÄ±n\n",
    "X_test_rfecv = rfecv.transform(X_test)\n",
    "y_pred = logreg.predict(X_test_rfecv)\n",
    "\n",
    "# DoÄŸruluk skorunu deÄŸerlendirin\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Model Accuracy with RFECV: {accuracy:.4f}\")\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_rfecv)\n",
    "X_test_scaled = scaler.transform(X_test_rfecv)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = logreg.predict(X_test_scaled)\n",
    "\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "print(f\"Logistic Regression Model Accuracy with RFECV and Scaling: {accuracy_scaled:.4f}\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Model Accuracy: {accuracy_rf:.4f}\")\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test_scaled)\n",
    "\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(f\"Gradient Boosting Model Accuracy: {accuracy_gb:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9afc2e-686a-469c-b5fa-95bc98a284c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Ã–nceden belirlenmiÅŸ X ve y'yi kullanalÄ±m\n",
    "# X: Ã–zellik matrisi, y: Hedef deÄŸiÅŸken\n",
    "# Ã–rneÄŸin, new_X ve new_y'yi kullanalÄ±m\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression modeli\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# RFECV'yi kullanarak Ã¶zellik seÃ§imi yapÄ±n\n",
    "rfecv = RFECV(estimator=logreg, step=1, cv=5, scoring='accuracy')  # cv=5, 5-fold cross-validation kullanÄ±yoruz\n",
    "X_train_rfecv = rfecv.fit_transform(X_train, y_train)\n",
    "\n",
    "# Modeli eÄŸitin\n",
    "logreg.fit(X_train_rfecv, y_train)\n",
    "\n",
    "# Test seti Ã¼zerinde tahminler yapÄ±n\n",
    "X_test_rfecv = rfecv.transform(X_test)\n",
    "y_pred = logreg.predict(X_test_rfecv)\n",
    "\n",
    "# DoÄŸruluk skorunu deÄŸerlendirin\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Model Accuracy with RFECV: {accuracy:.4f}\")\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_rfecv)\n",
    "X_test_scaled = scaler.transform(X_test_rfecv)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = logreg.predict(X_test_scaled)\n",
    "\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "print(f\"Logistic Regression Model Accuracy with RFECV and Scaling: {accuracy_scaled:.4f}\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Model Accuracy with MinMax Scaling: {accuracy_rf:.4f}\")\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test_scaled)\n",
    "\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(f\"Gradient Boosting Model Accuracy with MinMax Scaling: {accuracy_gb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a1ce8d-1885-4cd1-922b-c6aac67d6f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Extract features and target variable\n",
    "X = imputed_df.drop('Q16', axis=1)\n",
    "y = imputed_df['Q16']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "##################################################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have already calculated class weights\n",
    "class_weight_dict = {'A': 0.5246993127147767, 'B': 1.0078382838283828, 'C': 1.6918282548476453, 'D': 1.9575320512820513}\n",
    "class_counts = Counter(y)\n",
    "weighted_counts_df = pd.DataFrame({'Class': list(class_counts.keys()), 'Weighted Count': [class_counts[class_] * weight for class_, weight in class_weight_dict.items()]})\n",
    "print(\"\\nWeighted Class Counts:\")\n",
    "print(weighted_counts_df)\n",
    "# Random Forest Classifier with class weights\n",
    "rf_classifier = RandomForestClassifier(random_state=42, class_weight=class_weight_dict)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy with Class Weights: {accuracy_rf:.4f}\")\n",
    "\n",
    "# Support Vector Classifier (SVC) with class weights\n",
    "svc_classifier = SVC(random_state=42, class_weight=class_weight_dict)\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "y_pred_svc = svc_classifier.predict(X_test)\n",
    "accuracy_svc = accuracy_score(y_test, y_pred_svc)\n",
    "print(f\"SVC Accuracy with Class Weights: {accuracy_svc:.4f}\")\n",
    "\n",
    "##################################################################\n",
    "\n",
    "\n",
    "# Logistic Regression modeli\n",
    "logreg = LogisticRegression(max_iter=1000, class_weight=class_weight_dict)\n",
    "\n",
    "# RFECV'yi kullanarak Ã¶zellik seÃ§imi yapÄ±n\n",
    "rfecv = RFECV(estimator=logreg, step=1, cv=5, scoring='accuracy')  # cv=5, 5-fold cross-validation kullanÄ±yoruz\n",
    "X_train_rfecv = rfecv.fit_transform(X_train, y_train)\n",
    "\n",
    "quantile_transformer = QuantileTransformer(output_distribution='normal',random_state=0)\n",
    "X_train_quantile = quantile_transformer.fit_transform(X_train_rfecv)\n",
    "X_test_quantile = quantile_transformer.transform(rfecv.transform(X_test))\n",
    "\n",
    "\n",
    "# Modeli eÄŸitin\n",
    "logreg.fit(X_train_quantile, y_train)\n",
    "\n",
    "# Test seti Ã¼zerinde tahminler yapÄ±n\n",
    "X_test_rfecv = rfecv.transform(X_test)\n",
    "y_pred = logreg.predict(X_test_quantile)\n",
    "\n",
    "# DoÄŸruluk skorunu deÄŸerlendirin\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Model Accuracy with RFECV: {accuracy:.4f}\")\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_quantile)\n",
    "X_test_scaled = scaler.transform(X_test_quantile)\n",
    "\n",
    "\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = logreg.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "print(f\"Logistic Regression Model Accuracy with RFECV and Scaling: {accuracy_scaled:.4f}\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Model Accuracy with MinMax Scaling: {accuracy_rf:.4f}\")\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test_scaled)\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(f\"Gradient Boosting Model Accuracy with MinMax Scaling: {accuracy_gb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0758ec-2671-4a03-9013-e2be640cad7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
